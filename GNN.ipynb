{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit\n",
        "!pip install deepchem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap63eMNKI_Qk",
        "outputId": "4fd34ae6-e433-4730-d771-55f0a33f7415"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (2023.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Collecting deepchem\n",
            "  Downloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.2/693.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.2)\n",
            "Collecting scipy<1.9 (from deepchem)\n",
            "  Downloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (from deepchem) (2023.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2023.3.post1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->deepchem) (9.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->deepchem) (1.16.0)\n",
            "Installing collected packages: scipy, deepchem\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.2\n",
            "    Uninstalling scipy-1.11.2:\n",
            "      Successfully uninstalled scipy-1.11.2\n",
            "Successfully installed deepchem-2.7.1 scipy-1.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjFH0lQrIr_4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "import deepchem as dc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "train_data = pd.read_csv('train_linear.csv')\n",
        "\n",
        "# Define constants\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 200\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Extract SMILES strings and convert to molecular graphs\n",
        "def smiles_to_graph(smiles_list):\n",
        "    graph_representations = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in smiles_list]\n",
        "    return np.array(graph_representations)\n",
        "\n",
        "train_graph_representations = smiles_to_graph(train_data[\"SMILES\"])\n",
        "feature_columns = train_data.columns.difference([\"id\", \"SMILES\", \"MLM\", \"HLM\"])\n",
        "\n",
        "# Normalize other features from the dataset\n",
        "scaler = StandardScaler().fit(train_data[feature_columns])\n",
        "normalized_features = scaler.transform(train_data[feature_columns])\n",
        "\n",
        "# Combine molecular representations and normalized features\n",
        "combined_train_features = np.hstack([train_graph_representations, normalized_features])\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_features, val_features, train_labels, val_labels = train_test_split(combined_train_features, train_data[['MLM', 'HLM']].values, test_size=0.1, random_state=42)\n",
        "\n",
        "# Define PyTorch dataset and dataloader\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels=None):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.labels is None:\n",
        "            return torch.tensor(self.features[idx], dtype=torch.float)\n",
        "        else:\n",
        "            return torch.tensor(self.features[idx], dtype=torch.float), torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "\n",
        "train_dataset = CustomDataset(train_features, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataset = CustomDataset(val_features, val_labels)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Define the neural network model\n",
        "class MultiInputNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MultiInputNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 2)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "model = MultiInputNN(train_features.shape[1])\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, (features, labels) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Validate on the validation set\n",
        "    model.eval()\n",
        "    val_predictions = []\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for features, labels in val_dataloader:\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            val_predictions.append(outputs.numpy())\n",
        "\n",
        "    val_predictions = np.vstack(val_predictions)\n",
        "    rmse = np.sqrt(mean_squared_error(val_labels, val_predictions))\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Training Loss: {total_loss / len(train_dataloader):.4f}, Validation RMSE: {rmse:.4f}\")\n",
        "\n",
        "# Load test data, featurize, and make predictions\n",
        "test_data = pd.read_csv('test_linear.csv')\n",
        "test_graph_representations = smiles_to_graph(test_data[\"SMILES\"])\n",
        "normalized_test_features = scaler.transform(test_data[feature_columns])\n",
        "combined_test_features = np.hstack([test_graph_representations, normalized_test_features])\n",
        "test_dataset = CustomDataset(combined_test_features)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "with torch.no_grad():\n",
        "    for features in test_dataloader:\n",
        "        outputs = model(features)\n",
        "        test_predictions.append(outputs.numpy())\n",
        "\n",
        "test_predictions = np.vstack(test_predictions)\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test_data[\"id\"],\n",
        "    \"MLM\": test_predictions[:, 0],\n",
        "    \"HLM\": test_predictions[:, 1]\n",
        "})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"Submission file created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "train_data = pd.read_csv('train_linear.csv')\n",
        "\n",
        "# Constants\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 200\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 1e-5\n",
        "PATIENCE = 10\n",
        "\n",
        "def smiles_to_graph(smiles_list):\n",
        "    graph_representations = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in smiles_list]\n",
        "    return np.array(graph_representations)\n",
        "\n",
        "train_graph_representations = smiles_to_graph(train_data[\"SMILES\"])\n",
        "feature_columns = train_data.columns.difference([\"id\", \"SMILES\", \"MLM\", \"HLM\"])\n",
        "scaler = StandardScaler().fit(train_data[feature_columns])\n",
        "normalized_features = scaler.transform(train_data[feature_columns])\n",
        "combined_train_features = np.hstack([train_graph_representations, normalized_features])\n",
        "train_features, val_features, train_labels, val_labels = train_test_split(combined_train_features, train_data[['MLM', 'HLM']].values, test_size=0.1, random_state=42)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels=None):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.labels is None:\n",
        "            return torch.tensor(self.features[idx], dtype=torch.float)\n",
        "        else:\n",
        "            return torch.tensor(self.features[idx], dtype=torch.float), torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "\n",
        "train_dataset = CustomDataset(train_features, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataset = CustomDataset(val_features, val_labels)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features, out_features)\n",
        "        self.fc2 = nn.Linear(out_features, out_features)\n",
        "        self.bn1 = nn.BatchNorm1d(out_features)\n",
        "        self.bn2 = nn.BatchNorm1d(out_features)\n",
        "        self.activation = nn.LeakyReLU(negative_slope=0.01)\n",
        "        self.shortcut = nn.Linear(in_features, out_features) if in_features != out_features else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.fc1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.shortcut is not None:\n",
        "            identity = self.shortcut(x)\n",
        "        out += identity\n",
        "        out = self.activation(out)\n",
        "        return out\n",
        "\n",
        "class ModifiedMultiInputNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ModifiedMultiInputNN, self).__init__()\n",
        "        self.block1 = ResidualBlock(input_dim, 512)\n",
        "        self.block2 = ResidualBlock(512, 256)\n",
        "        self.block3 = ResidualBlock(256, 128)\n",
        "        self.fc_out = nn.Linear(128, 2)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "model = ModifiedMultiInputNN(train_features.shape[1])\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=PATIENCE, verbose=True)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, (features, labels) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    val_predictions = []\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for features, labels in val_dataloader:\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            val_predictions.append(outputs.numpy())\n",
        "\n",
        "    val_predictions = np.vstack(val_predictions)\n",
        "    rmse = np.sqrt(mean_squared_error(val_labels, val_predictions))\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Training Loss: {total_loss / len(train_dataloader):.4f}, Validation RMSE: {rmse:.4f}\")\n",
        "\n",
        "test_data = pd.read_csv('test_linear.csv')\n",
        "test_graph_representations = smiles_to_graph(test_data[\"SMILES\"])\n",
        "normalized_test_features = scaler.transform(test_data[feature_columns])\n",
        "combined_test_features = np.hstack([test_graph_representations, normalized_test_features])\n",
        "test_dataset = CustomDataset(combined_test_features)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "with torch.no_grad():\n",
        "    for features in test_dataloader:\n",
        "        outputs = model(features)\n",
        "        test_predictions.append(outputs.numpy())\n",
        "\n",
        "test_predictions = np.vstack(test_predictions)\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test_data[\"id\"],\n",
        "    \"MLM\": test_predictions[:, 0],\n",
        "    \"HLM\": test_predictions[:, 1]\n",
        "})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"Submission file created!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xp-NFR0NcpFx",
        "outputId": "fd912aa8-fe8d-4b91-e4af-f46b8c3363a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Training Loss: 1478.5110, Validation RMSE: 32.9110\n",
            "Epoch 2/200, Training Loss: 825.3048, Validation RMSE: 33.9291\n",
            "Epoch 3/200, Training Loss: 608.4586, Validation RMSE: 34.0272\n",
            "Epoch 4/200, Training Loss: 448.9705, Validation RMSE: 34.3051\n",
            "Epoch 5/200, Training Loss: 348.7811, Validation RMSE: 35.3628\n",
            "Epoch 6/200, Training Loss: 290.9373, Validation RMSE: 35.5880\n",
            "Epoch 7/200, Training Loss: 243.2873, Validation RMSE: 36.2125\n",
            "Epoch 8/200, Training Loss: 210.2083, Validation RMSE: 36.8698\n",
            "Epoch 9/200, Training Loss: 198.6118, Validation RMSE: 35.9928\n",
            "Epoch 10/200, Training Loss: 172.5647, Validation RMSE: 35.9928\n",
            "Epoch 11/200, Training Loss: 157.4021, Validation RMSE: 37.6858\n",
            "Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 12/200, Training Loss: 143.3893, Validation RMSE: 35.5661\n",
            "Epoch 13/200, Training Loss: 116.3217, Validation RMSE: 35.8469\n",
            "Epoch 14/200, Training Loss: 94.2126, Validation RMSE: 35.5777\n",
            "Epoch 15/200, Training Loss: 93.8154, Validation RMSE: 35.8401\n",
            "Epoch 16/200, Training Loss: 90.8402, Validation RMSE: 35.7132\n",
            "Epoch 17/200, Training Loss: 85.9416, Validation RMSE: 35.8245\n",
            "Epoch 18/200, Training Loss: 80.8807, Validation RMSE: 35.6334\n",
            "Epoch 19/200, Training Loss: 79.1569, Validation RMSE: 36.0932\n",
            "Epoch 20/200, Training Loss: 75.1923, Validation RMSE: 35.7569\n",
            "Epoch 21/200, Training Loss: 79.8198, Validation RMSE: 36.3969\n",
            "Epoch 22/200, Training Loss: 74.9152, Validation RMSE: 36.0379\n",
            "Epoch 00023: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch 23/200, Training Loss: 71.9567, Validation RMSE: 36.3018\n",
            "Epoch 24/200, Training Loss: 66.8355, Validation RMSE: 35.6154\n",
            "Epoch 25/200, Training Loss: 61.2452, Validation RMSE: 35.5292\n",
            "Epoch 26/200, Training Loss: 58.1266, Validation RMSE: 35.8880\n",
            "Epoch 27/200, Training Loss: 58.6439, Validation RMSE: 35.8223\n",
            "Epoch 28/200, Training Loss: 56.7636, Validation RMSE: 35.9832\n",
            "Epoch 29/200, Training Loss: 55.8048, Validation RMSE: 35.7359\n",
            "Epoch 30/200, Training Loss: 56.4218, Validation RMSE: 35.6967\n",
            "Epoch 31/200, Training Loss: 55.6299, Validation RMSE: 35.8876\n",
            "Epoch 32/200, Training Loss: 55.5130, Validation RMSE: 35.8619\n",
            "Epoch 33/200, Training Loss: 51.7275, Validation RMSE: 36.3276\n",
            "Epoch 00034: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch 34/200, Training Loss: 52.8092, Validation RMSE: 35.6092\n",
            "Epoch 35/200, Training Loss: 51.7024, Validation RMSE: 35.7752\n",
            "Epoch 36/200, Training Loss: 49.7056, Validation RMSE: 35.7099\n",
            "Epoch 37/200, Training Loss: 46.1601, Validation RMSE: 35.6713\n",
            "Epoch 38/200, Training Loss: 49.2175, Validation RMSE: 35.9798\n",
            "Epoch 39/200, Training Loss: 45.4385, Validation RMSE: 35.8162\n",
            "Epoch 40/200, Training Loss: 46.7002, Validation RMSE: 35.8568\n",
            "Epoch 41/200, Training Loss: 47.7776, Validation RMSE: 35.8391\n",
            "Epoch 42/200, Training Loss: 48.1023, Validation RMSE: 35.7594\n",
            "Epoch 43/200, Training Loss: 45.4306, Validation RMSE: 35.9634\n",
            "Epoch 44/200, Training Loss: 42.4796, Validation RMSE: 35.9095\n",
            "Epoch 00045: reducing learning rate of group 0 to 6.2500e-05.\n",
            "Epoch 45/200, Training Loss: 45.5528, Validation RMSE: 36.2517\n",
            "Epoch 46/200, Training Loss: 44.1204, Validation RMSE: 35.8636\n",
            "Epoch 47/200, Training Loss: 42.1140, Validation RMSE: 36.0048\n",
            "Epoch 48/200, Training Loss: 42.8526, Validation RMSE: 35.9640\n",
            "Epoch 49/200, Training Loss: 41.7652, Validation RMSE: 35.9265\n",
            "Epoch 50/200, Training Loss: 40.3988, Validation RMSE: 35.9174\n",
            "Epoch 51/200, Training Loss: 42.4553, Validation RMSE: 35.9694\n",
            "Epoch 52/200, Training Loss: 40.7025, Validation RMSE: 36.0505\n",
            "Epoch 53/200, Training Loss: 43.1234, Validation RMSE: 36.0288\n",
            "Epoch 54/200, Training Loss: 41.3065, Validation RMSE: 35.7993\n",
            "Epoch 55/200, Training Loss: 42.3743, Validation RMSE: 35.9120\n",
            "Epoch 00056: reducing learning rate of group 0 to 3.1250e-05.\n",
            "Epoch 56/200, Training Loss: 38.3108, Validation RMSE: 36.0102\n",
            "Epoch 57/200, Training Loss: 39.8606, Validation RMSE: 35.9103\n",
            "Epoch 58/200, Training Loss: 41.0690, Validation RMSE: 35.9594\n",
            "Epoch 59/200, Training Loss: 45.3689, Validation RMSE: 35.7854\n",
            "Epoch 60/200, Training Loss: 38.5987, Validation RMSE: 35.9474\n",
            "Epoch 61/200, Training Loss: 39.7791, Validation RMSE: 35.9185\n",
            "Epoch 62/200, Training Loss: 39.8762, Validation RMSE: 35.8919\n",
            "Epoch 63/200, Training Loss: 39.8518, Validation RMSE: 36.0097\n",
            "Epoch 64/200, Training Loss: 39.1202, Validation RMSE: 35.7860\n",
            "Epoch 65/200, Training Loss: 37.0980, Validation RMSE: 35.8390\n",
            "Epoch 66/200, Training Loss: 38.9212, Validation RMSE: 35.8950\n",
            "Epoch 00067: reducing learning rate of group 0 to 1.5625e-05.\n",
            "Epoch 67/200, Training Loss: 39.4107, Validation RMSE: 35.9481\n",
            "Epoch 68/200, Training Loss: 39.0047, Validation RMSE: 35.8511\n",
            "Epoch 69/200, Training Loss: 37.5702, Validation RMSE: 35.8263\n",
            "Epoch 70/200, Training Loss: 37.7508, Validation RMSE: 35.8354\n",
            "Epoch 71/200, Training Loss: 37.4061, Validation RMSE: 35.7007\n",
            "Epoch 72/200, Training Loss: 39.4060, Validation RMSE: 35.8069\n",
            "Epoch 73/200, Training Loss: 37.9207, Validation RMSE: 35.6682\n",
            "Epoch 74/200, Training Loss: 38.3731, Validation RMSE: 35.9174\n",
            "Epoch 75/200, Training Loss: 38.4756, Validation RMSE: 35.9002\n",
            "Epoch 76/200, Training Loss: 37.7421, Validation RMSE: 35.8153\n",
            "Epoch 77/200, Training Loss: 38.6583, Validation RMSE: 35.8105\n",
            "Epoch 00078: reducing learning rate of group 0 to 7.8125e-06.\n",
            "Epoch 78/200, Training Loss: 36.6584, Validation RMSE: 35.8357\n",
            "Epoch 79/200, Training Loss: 36.9518, Validation RMSE: 35.8191\n",
            "Epoch 80/200, Training Loss: 37.4194, Validation RMSE: 35.6515\n",
            "Epoch 81/200, Training Loss: 37.0690, Validation RMSE: 35.7258\n",
            "Epoch 82/200, Training Loss: 36.9752, Validation RMSE: 35.8523\n",
            "Epoch 83/200, Training Loss: 37.9505, Validation RMSE: 35.9641\n",
            "Epoch 84/200, Training Loss: 37.0190, Validation RMSE: 35.7674\n",
            "Epoch 85/200, Training Loss: 38.6481, Validation RMSE: 35.8242\n",
            "Epoch 86/200, Training Loss: 38.1178, Validation RMSE: 35.9679\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f61bdfd51528>\u001b[0m in \u001b[0;36m<cell line: 98>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f61bdfd51528>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f61bdfd51528>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "train_data = pd.read_csv('train_linear.csv')\n",
        "\n",
        "# Constants\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 200\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 1e-5\n",
        "PATIENCE = 10\n",
        "EARLY_STOPPING_PATIENCE = 20\n",
        "\n",
        "def smiles_to_graph(smiles_list):\n",
        "    graph_representations = [AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 2, nBits=1024) for smiles in smiles_list]\n",
        "    return np.array(graph_representations)\n",
        "\n",
        "train_graph_representations = smiles_to_graph(train_data[\"SMILES\"])\n",
        "feature_columns = train_data.columns.difference([\"id\", \"SMILES\", \"MLM\", \"HLM\"])\n",
        "scaler = StandardScaler().fit(train_data[feature_columns])\n",
        "normalized_features = scaler.transform(train_data[feature_columns])\n",
        "combined_train_features = np.hstack([train_graph_representations, normalized_features])\n",
        "train_features, val_features, train_labels, val_labels = train_test_split(combined_train_features, train_data[['MLM', 'HLM']].values, test_size=0.1, random_state=42)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels=None):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.labels is None:\n",
        "            return torch.tensor(self.features[idx], dtype=torch.float)\n",
        "        else:\n",
        "            return torch.tensor(self.features[idx], dtype=torch.float), torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "\n",
        "train_dataset = CustomDataset(train_features, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataset = CustomDataset(val_features, val_labels)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Define a simpler neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 2)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.activation = nn.LeakyReLU(negative_slope=0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleNN(train_features.shape[1])\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=PATIENCE, verbose=True)\n",
        "\n",
        "# Early stopping parameters\n",
        "epochs_without_improvement = 0\n",
        "best_val_rmse = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, (features, labels) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    val_predictions = []\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for features, labels in val_dataloader:\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            val_predictions.append(outputs.numpy())\n",
        "\n",
        "    val_predictions = np.vstack(val_predictions)\n",
        "    rmse = np.sqrt(mean_squared_error(val_labels, val_predictions))\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Check for early stopping\n",
        "    if rmse < best_val_rmse:\n",
        "        best_val_rmse = rmse\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "\n",
        "    if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:\n",
        "        print(\"Early stopping due to no improvement in validation RMSE.\")\n",
        "        break\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Training Loss: {total_loss / len(train_dataloader):.4f}, Validation RMSE: {rmse:.4f}\")\n",
        "\n",
        "test_data = pd.read_csv('test_linear.csv')\n",
        "test_graph_representations = smiles_to_graph(test_data[\"SMILES\"])\n",
        "normalized_test_features = scaler.transform(test_data[feature_columns])\n",
        "combined_test_features = np.hstack([test_graph_representations, normalized_test_features])\n",
        "test_dataset = CustomDataset(combined_test_features)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "with torch.no_grad():\n",
        "    for features in test_dataloader:\n",
        "        outputs = model(features)\n",
        "        test_predictions.append(outputs.numpy())\n",
        "\n",
        "test_predictions = np.vstack(test_predictions)\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test_data[\"id\"],\n",
        "    \"MLM\": test_predictions[:, 0],\n",
        "    \"HLM\": test_predictions[:, 1]\n",
        "})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"Submission file created!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIkdB9KU0iKC",
        "outputId": "6bc1a857-484d-48fd-c35f-b76c7be57678"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Training Loss: 1531.3347, Validation RMSE: 32.9456\n",
            "Epoch 2/200, Training Loss: 1042.0308, Validation RMSE: 33.0084\n",
            "Epoch 3/200, Training Loss: 941.4246, Validation RMSE: 33.3867\n",
            "Epoch 4/200, Training Loss: 855.7310, Validation RMSE: 33.5276\n",
            "Epoch 5/200, Training Loss: 759.3003, Validation RMSE: 34.7648\n",
            "Epoch 6/200, Training Loss: 675.6714, Validation RMSE: 34.6236\n",
            "Epoch 7/200, Training Loss: 587.5877, Validation RMSE: 34.7195\n",
            "Epoch 8/200, Training Loss: 482.4557, Validation RMSE: 36.0319\n",
            "Epoch 9/200, Training Loss: 430.7852, Validation RMSE: 35.8163\n",
            "Epoch 10/200, Training Loss: 378.6470, Validation RMSE: 35.6597\n",
            "Epoch 11/200, Training Loss: 339.6004, Validation RMSE: 35.9257\n",
            "Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 12/200, Training Loss: 308.1426, Validation RMSE: 35.6516\n",
            "Epoch 13/200, Training Loss: 274.2221, Validation RMSE: 36.0118\n",
            "Epoch 14/200, Training Loss: 241.1260, Validation RMSE: 36.1608\n",
            "Epoch 15/200, Training Loss: 223.8295, Validation RMSE: 36.0943\n",
            "Epoch 16/200, Training Loss: 211.4575, Validation RMSE: 36.2165\n",
            "Epoch 17/200, Training Loss: 197.1089, Validation RMSE: 36.4734\n",
            "Epoch 18/200, Training Loss: 186.9585, Validation RMSE: 36.5934\n",
            "Epoch 19/200, Training Loss: 178.3507, Validation RMSE: 36.6963\n",
            "Epoch 20/200, Training Loss: 167.1889, Validation RMSE: 36.3506\n",
            "Early stopping due to no improvement in validation RMSE.\n",
            "Submission file created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "55GPQ8px2qIe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}